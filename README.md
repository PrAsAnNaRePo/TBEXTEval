# TBEXTEval

As the name suggest, its a benchmarking framework for evaluating VLLMs for table extraction. Our internal dataset contains several samples for complex *structured*, *multi-lingual* and *cluttered data* that are hard to extract, that really helps to test the large models on this specific domain.

### Assumptions:
Here `agent` refers to the model which you want to test with the dataset.
`evaluator` is the model that evaluates and create benchmarks for the agent output.

## Get started

**File structure**

- tbexteval/
    - agents/
        - (different agents that works with openai, anthropic)
    - evaluator/
        - (evalutor class that calculate metrics from outputs generated by agent)
    - data/
        - (contains dataset to run benchmark)
    - logs/
        - (all the logs file for the benchmark will be inside this folder)
- run_benchmark.py

### Collect data :
You can bring your own data (i.e., image with table contents) inside the `tbext/data/cropped_images/`.

### Run the benchmark :
- select your agent model, such as models from `openai` and `claude` (extending the model soon).
- select your evaluator model, such as models from `openai` and `claude` (extending the model soon).
- Set up your system prompt in the file `tbexteval/prompts/gen_prompt.txt`, **NOTE:** its not recommended to change the `eval_prompt`.
- run the scipt with
```bash
python run_benchmark.py --model_name <agent_model_name_here> --eval_model_name <eval_model_name_here>
 ```
Other params:
 - `temperature`, default to 0
 - `output_path`, log name path (defualt to model_name)
 - `data_path`, path to the dataset (default to `tbexteval/data/cropp_tool/cropped_images`)